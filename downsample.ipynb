{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea 1: Using scipy resample functions\n",
    "Problems with this is that although the data itself gets downsampled, attributes such as time axis and sampling frequency remain the same.\n",
    "- resample_poly can downsample only by an integer factor\n",
    "- resample can downsample for any factor (you just need to specify the number of samples)\n",
    "- decimate can downsample only by an integer factor\n",
    "\n",
    "This is important to consider if the data is not just being downsampled by half (10kHz -> 5kHz), but also to other frequencies such as 4kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import resample_poly, decimate, resample\n",
    "from matplotlib import pyplot as plt\n",
    "import h5py\n",
    "import dascore as dc\n",
    "import shutil\n",
    "from dascore.io.neubrex.utils_das import _is_neubrex as _is_neubrex_das\n",
    "\n",
    "from scipy.stats import ks_2samp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample Poly\n",
    "x = np.random.randn(100)\n",
    "print(x)\n",
    "y = resample_poly(x,up=1,down=2,axis=0, window=[1])\n",
    "print(y)\n",
    "\n",
    "# Resample\n",
    "y = resample(x, num=len(x)//2)\n",
    "print(y)\n",
    "\n",
    "# Decimate\n",
    "y = decimate(x,q=2,zero_phase=True)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Basic resampling of hd5 file\n",
    "method1_path = 'example_data/16B_StrainRate_20240407T072116+0000_34572_method1.h5'\n",
    "og_path = 'example_data/16B_StrainRate_20240407T072116+0000_34572.h5'\n",
    "\n",
    "shutil.copy2(og_path, method1_path)\n",
    "\n",
    "with h5py.File(method1_path, 'r+') as f:\n",
    "    dataset = f['Acoustic']\n",
    "    resampled_dataset = resample(dataset, num=dataset.shape[0]//2, axis=0)\n",
    "    # resampled_dataset = resample_poly(dataset, up=1, down=2, axis=0)\n",
    "    dataset.resize(resampled_dataset.shape)\n",
    "    dataset[...] = resampled_dataset\n",
    "    dataset.attrs.modify('TimeSamplingInterval(seconds)', dataset.attrs['TimeSamplingInterval(seconds)']*2)\n",
    "    dataset.attrs.modify('InterrogationRate(Hz)', dataset.attrs['InterrogationRate(Hz)']/2)\n",
    "\n",
    "    # for key in dataset.attrs:\n",
    "        # print(f\"{key}: {dataset.attrs[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Concatenate three patches, filter, and decimate and then take only the middle one\n",
    "file1_path = 'example_data/16B_StrainRate_20240407T072116+0000_34572.h5'\n",
    "file2_path = 'example_data/16B_StrainRate_20240407T072128+0000_34573.h5'\n",
    "file3_path = 'example_data/16B_StrainRate_20240407T072140+0000_34574.h5'\n",
    "file2_path_m2 = 'example_data/16B_StrainRate_20240407T072128+0000_34573_method2.h5'\n",
    "\n",
    "shutil.copy2(file2_path, file2_path_m2)\n",
    "\n",
    "f1 = h5py.File(file1_path, 'r')\n",
    "f2 = h5py.File(file2_path_m2, 'r+')\n",
    "f3 = h5py.File(file3_path, 'r')\n",
    "\n",
    "dataset1 = f1['Acoustic']\n",
    "dataset2 = f2['Acoustic']\n",
    "dataset3 = f3['Acoustic']\n",
    "\n",
    "datasets_data = np.concatenate([dataset1, dataset2, dataset3], axis=0)\n",
    "data_downsampled = resample(datasets_data, num=datasets_data.shape[0]//2, axis=0)\n",
    "# data_downsampled = resample_poly(datasets_data, up=1, down=2, axis=0)\n",
    "data_downsampled = data_downsampled[dataset1.shape[0]//2:dataset1.shape[0]//2+dataset2.shape[0]//2]\n",
    "\n",
    "dataset2.resize(data_downsampled.shape)\n",
    "dataset2[...] = data_downsampled\n",
    "dataset2.attrs.modify('TimeSamplingInterval(seconds)', dataset2.attrs['TimeSamplingInterval(seconds)']*2)\n",
    "dataset2.attrs.modify('InterrogationRate(Hz)', dataset2.attrs['InterrogationRate(Hz)']/2) \n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see size\n",
    "with h5py.File(og_path, 'r') as f:\n",
    "    dataset = f['Acoustic']\n",
    "    print(\"Original: \", dataset.shape)\n",
    "\n",
    "with h5py.File(method1_path, 'r') as f:\n",
    "    dataset = f['Acoustic']\n",
    "    print(\"Method 1: \", dataset.shape)\n",
    "\n",
    "with h5py.File(file2_path_m2, 'r') as f:\n",
    "    dataset = f['Acoustic']\n",
    "    print(\"Method 2: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the resampled file with dascore to check if the file attributes are still valid\n",
    "# Original file\n",
    "print(\"Original file: \", og_path)\n",
    "patch_og = dc.spool(og_path)[0].transpose()\n",
    "# patch_og.viz.waterfall(show=True, scale=0.1)\n",
    "\n",
    "print(dc.get_format(og_path))\n",
    "print(patch_og.attrs)\n",
    "\n",
    "# Method 1: Visualization\n",
    "print(\"Method 1 file: \", method1_path)\n",
    "patch = dc.spool(method1_path)[0].transpose()\n",
    "# patch.viz.waterfall(show=True, scale=0.1)\n",
    "# patch.attrs.coords['time']\n",
    "\n",
    "print(dc.get_format(method1_path))\n",
    "print(patch.attrs)\n",
    "\n",
    "# Method 2: Visualization\n",
    "print(\"Method 2 file: \", file2_path_m2)\n",
    "patch_m2 = dc.spool(file2_path_m2)[0].transpose()\n",
    "# patch_m2.viz.waterfall(show=True, scale=0.1)\n",
    "\n",
    "print(dc.get_format(file2_path_m2))\n",
    "print(patch_m2.attrs)\n",
    "\n",
    "# Validation: Check if dascore's decimate function leads to the same results\n",
    "print(\"Dascore decimate\")\n",
    "decimated_patch = patch_og.decimate(time=2)\n",
    "# decimated_patch.viz.waterfall(show=True, scale=0.1)\n",
    "\n",
    "print(decimated_patch.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency analysis of the downsampled signals\n",
    "# 1. Original\n",
    "print('Original Signal')\n",
    "fk_patch = patch_og.dft(patch.dims)\n",
    "ax = fk_patch.abs().viz.waterfall(scale=0.1, cmap='inferno')\n",
    "ax.set_title('Original Signal')\n",
    "\n",
    "# 2. Method 1\n",
    "print('Method 1 Signal')\n",
    "fk_patch = patch.dft(patch.dims)\n",
    "ax = fk_patch.abs().viz.waterfall(scale=0.1, cmap='inferno')\n",
    "ax.set_title('Method 1 Signal')\n",
    "\n",
    "# 3. Method 2\n",
    "print('Method 2 Signal')\n",
    "fk_patch = patch_m2.dft(patch.dims)\n",
    "ax = fk_patch.abs().viz.waterfall(scale=0.1, cmap='inferno')\n",
    "ax.set_title('Method 2 Signal')\n",
    "\n",
    "# 4. Dascore decimate\n",
    "print('Dascore Decimate')\n",
    "fk_patch = decimated_patch.dft(patch.dims)\n",
    "ax = fk_patch.abs().viz.waterfall(scale=0.1, cmap='inferno')\n",
    "ax.set_title('Dascore Decimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency analysis channel-wise\n",
    "# 1. Original\n",
    "print('Original Signal')\n",
    "freq_patch_og = patch_og.dft(dim=\"time\").abs()\n",
    "freq_coords = freq_patch_og.coords.get_array(\"ft_time\")\n",
    "freq_patch_og = freq_patch_og.select(ft_time=(0, None))\n",
    "ax = freq_patch_og.viz.waterfall(show=True, scale=0.1)\n",
    "ax.set_title('Original Signal')\n",
    "\n",
    "# 2. Method 1\n",
    "print('Method 1 Signal')\n",
    "freq_patch_m1 = patch.dft(dim=\"time\").abs()\n",
    "freq_coords = freq_patch_m1.coords.get_array(\"ft_time\")\n",
    "freq_patch_m1 = freq_patch_m1.select(ft_time=(0, None))\n",
    "ax = freq_patch_m1.viz.waterfall(show=True, scale=0.1)\n",
    "ax.set_title('Method 1 Signal')\n",
    "\n",
    "# 3. Method 2\n",
    "print('Method 2 Signal')\n",
    "freq_patch_m2 = patch_m2.dft(dim=\"time\").abs()\n",
    "freq_coords = freq_patch_m2.coords.get_array(\"ft_time\")\n",
    "freq_patch_m2 = freq_patch_m2.select(ft_time=(0, None))\n",
    "ax = freq_patch_m2.viz.waterfall(show=True, scale=0.1)\n",
    "ax.set_title('Method 2 Signal')\n",
    "\n",
    "# 4. Dascore decimate\n",
    "print('Dascore Decimate')\n",
    "freq_patch_dc = decimated_patch.dft(dim=\"time\").abs()\n",
    "freq_coords = freq_patch_dc.coords.get_array(\"ft_time\")\n",
    "freq_patch_dc = freq_patch_dc.select(ft_time=(0, None))\n",
    "ax = freq_patch_dc.viz.waterfall(show=True, scale=0.1)\n",
    "ax.set_title('Dascore Decimate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourier spectrum error\n",
    "# 1. Method 1\n",
    "error1 = np.linalg.norm(freq_patch_og.data[:,:freq_patch_og.shape[1]//2] - freq_patch_m1.data)\n",
    "print(\"Method 1 Error: \", error1)\n",
    "\n",
    "# 2. Method 2\n",
    "error2 = np.linalg.norm(freq_patch_og.data[:,:freq_patch_og.shape[1]//2] - freq_patch_m2.data)\n",
    "print(\"Method 2 Error: \", error2)\n",
    "\n",
    "# 3. Dascore Decimate\n",
    "error3 = np.linalg.norm(freq_patch_og.data[:,:freq_patch_og.shape[1]//2] - freq_patch_dc.data)\n",
    "print(\"Dascore Decimate Error: \", error3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample and compute reconstruction error\n",
    "# 1. Method 1\n",
    "recon1 = resample_poly(patch.data, up=2, down=1, axis=1)\n",
    "error1 = np.linalg.norm(patch_og.data - recon1)\n",
    "\n",
    "# 2. Method 2\n",
    "recon2 = resample_poly(patch_m2.data, up=2, down=1, axis=1)\n",
    "error2 = np.linalg.norm(patch_og.data - recon2)\n",
    "\n",
    "# 3. Dascore decimate\n",
    "recon3 = resample_poly(decimated_patch.data, up=2, down=1, axis=1)\n",
    "error3 = np.linalg.norm(patch_og.data - recon3)\n",
    "\n",
    "print(\"Method 1 error: \", error1)\n",
    "print(\"Method 2 error: \", error2)\n",
    "print(\"Dascore decimate error: \", error3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error on on values that exist in upsampled signal\n",
    "error1 = np.linalg.norm(patch_og.data[:,::2] - patch.data)\n",
    "print(\"Method 1 error: \", error1, error1/patch.data.size)\n",
    "\n",
    "error2 = np.linalg.norm(patch_og.data[:,::2] - patch_m2.data)\n",
    "print(\"Method 2 error: \", error2, error2/patch_m2.data.size)\n",
    "\n",
    "error3 = np.linalg.norm(patch_og.data[:,::2] - decimated_patch.data)\n",
    "print(\"Dascore decimate error: \", error3, error3/decimated_patch.data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of values in downsampled and regular signals\n",
    "def plot_histogram(patch, title=\"Histogram of Patch Data\", bins=50):\n",
    "    data = patch.data\n",
    "    plt.hist(data.flatten(), bins=bins, edgecolor='black', density=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "bins = 100\n",
    "plot_histogram(patch_og, 'Original', bins)\n",
    "plot_histogram(patch, \"Method 1\", bins)\n",
    "plot_histogram(patch_m2, \"Method 2\", bins)\n",
    "plot_histogram(decimated_patch, \"Dascore\", bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import wilcoxon, ttest_rel\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the distributions have the same mean\n",
    "stat, p = ttest_ind(patch_og.data.flatten(), patch_m2.data.flatten(), equal_var=False)  # Set equal_var=True if variances are equal\n",
    "print(\"p-value:\", p)\n",
    "\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution mean-wise.\")\n",
    "else:\n",
    "    print(\"Likely from different distributions mean-wise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Sample Kolmogoov-Smirnov Test\n",
    "# This test might not make much sense since the assumption is that the data samples are independent, however\n",
    "# the second signal is a subsample of the first one, so they are highly correlated. Moreover, subsampling\n",
    "# changes the statistical properties of the signal, since high-frequency components are removed, lowering\n",
    "# the variance, which implies the different distributions.\n",
    "\n",
    "stat, p = ks_2samp(patch_og.data.flatten(), patch_m2.data.flatten())\n",
    "if p > 0.05:\n",
    "    print(\"Original and M2 likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Original and M2 likely from different distributions\")\n",
    "\n",
    "stat, p = ks_2samp(patch_og.data.flatten(), decimated_patch.data.flatten())\n",
    "if p > 0.05:\n",
    "    print(\"Original and Dascore likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Original and Dascore likely from different distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mann-Whitney U Test\n",
    "stat, p = mannwhitneyu(patch_og.data.flatten(), patch_m2.data.flatten())\n",
    "print(\"p-value:\", p)\n",
    "\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test\n",
    "stat, p = ttest_ind(patch_og.data.flatten(), patch_m2.data.flatten(), equal_var=False)  # Set equal_var=True if variances are equal\n",
    "print(\"p-value:\", p)\n",
    "\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Test relative\n",
    "stat, p = ttest_rel(patch_og.data[:,::2].flatten(), patch_m2.data.flatten())\n",
    "print(\"p-value:\", p)\n",
    "\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon test\n",
    "stat, p = wilcoxon(patch_og.data[:,::2].flatten(), patch.data.flatten())\n",
    "print(\"p-value:\", p)\n",
    "\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chisquare test\n",
    "bins = 100\n",
    "hist_og, bin_edges = np.histogram(patch_og.data.flatten(), bins=bins)\n",
    "hist_m1 = np.histogram(patch.data.flatten(), bins=bin_edges)[0]\n",
    "hist_m2 = np.histogram(patch_m2.data.flatten(), bins=bin_edges)[0]\n",
    "hist_dc = np.histogram(decimated_patch.data.flatten(), bins=bin_edges)[0]\n",
    "\n",
    "hist_og = hist_og/sum(hist_og)\n",
    "hist_m1 = hist_m1/sum(hist_m1)\n",
    "hist_m2 = hist_m2/sum(hist_m2)\n",
    "hist_dc = hist_dc/sum(hist_dc)\n",
    "\n",
    "print(\"METHOD 1:\")\n",
    "res = chisquare(hist_m1, hist_og)\n",
    "stat, p = res.statistic, res.pvalue\n",
    "print(\"Stat, p-value: \", stat, p)\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")\n",
    "\n",
    "print(\"METHOD 2:\")\n",
    "res = chisquare(hist_m2, hist_og)\n",
    "stat, p = res.statistic, res.pvalue\n",
    "print(\"Stat, p-value: \", stat, p)\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")\n",
    "\n",
    "print(\"METHOD DC:\")\n",
    "res = chisquare(hist_dc, hist_og)\n",
    "stat, p = res.statistic, res.pvalue\n",
    "print(\"Stat, p-value: \", stat, p)\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")\n",
    "\n",
    "print(\"M2 - DC\")\n",
    "res = chisquare(hist_dc, hist_m2)\n",
    "stat, p = res.statistic, res.pvalue\n",
    "print(\"Stat, p-value: \", stat, p)\n",
    "if p > 0.05:\n",
    "    print(\"Likely from the same distribution\")\n",
    "else:\n",
    "    print(\"Likely from different distributions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dascore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
